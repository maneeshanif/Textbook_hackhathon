---
title: "3.1.4 â€” Sensor Fusion"
sidebar_label: "3.1.4 â€” Sensor Fusion"
sidebar_position: 4
description: "Kalman filtering, complementary filters, and multi-sensor state estimation"
module: 3
week: 11
section: 4
tags: [sensor-fusion, kalman-filter, state-estimation, complementary-filter, ekf]
difficulty: advanced
estimated_time: "4-5 hours"
---

:::info ØªØ±Ø¬Ù…Û Ø²ÛŒØ± Ø§Ù„ØªÙˆØ§Ø¡
ÛŒÛ ØµÙØ­Û ÙÛŒ Ø§Ù„Ø­Ø§Ù„ Ø§Ù†Ú¯Ø±ÛŒØ²ÛŒ Ø³Û’ Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº ØªØ±Ø¬Ù…Û Ú©ÛŒØ§ Ø¬Ø§ Ø±ÛØ§ ÛÛ’Û” Ù…Ú©Ù…Ù„ Ù…ÙˆØ§Ø¯ Ú©Û’ Ù„ÛŒÛ’ Ø¨Ø±Ø§Û Ú©Ø±Ù… Ø§Ù†Ú¯Ø±ÛŒØ²ÛŒ ÙˆØ±Ú˜Ù† Ø¯ÛŒÚ©Ú¾ÛŒÚºÛ”
:::


# 3.1.4 â€” Sensor Fusion

<DifficultyBadge level="intermediate" />

> **Summary**: Combine multiple noisy sensors for accurate, robust state estimation using Kalman filtering and complementary techniques.

## ğŸ¯ Learning Objectives

- Understand sensor fusion principles
- Implement Kalman filter for state estimation
- Apply Extended Kalman Filter (EKF) for nonlinear systems
- Fuse IMU, encoders, and vision data
- Build complete robot state estimator

---

## ğŸ“– Theory: Why Sensor Fusion?

### The Multi-Sensor Problem

**Individual Sensor Limitations:**
- **IMU**: Drifts over time, noisy accelerometer
- **Encoders**: No absolute reference, slip errors
- **Vision**: Occlusions, lighting sensitivity, latency
- **Force sensors**: Noise, temperature drift

**Sensor Fusion Benefits:**
- Combine complementary strengths
- Reduce noise and drift
- Handle sensor failures gracefully
- Improve accuracy and robustness

### Fusion Approaches

| Method | Complexity | Use Case | Real-Time? |
|--------|-----------|----------|------------|
| **Averaging** | Low | Homogeneous sensors | âœ… Yes |
| **Complementary Filter** | Low | IMU fusion | âœ… Yes |
| **Kalman Filter** | Medium | Linear systems | âœ… Yes |
| **Extended Kalman (EKF)** | High | Nonlinear systems | âœ… Yes |
| **Particle Filter** | Very High | Multi-modal distributions | âš ï¸ Slow |

---

## ğŸ¯ Part 1: Kalman Filter Basics

### The Kalman Filter

**Problem**: Estimate state $\mathbf{x}$ from noisy measurements $\mathbf{z}$.

**System Model:**
$$
\mathbf{x}_{k} = \mathbf{F} \mathbf{x}_{k-1} + \mathbf{B} \mathbf{u}_{k} + \mathbf{w}_k
$$
$$
\mathbf{z}_{k} = \mathbf{H} \mathbf{x}_{k} + \mathbf{v}_k
$$

Where:
- $\mathbf{F}$: State transition matrix
- $\mathbf{H}$: Observation matrix
- $\mathbf{w}_k \sim \mathcal{N}(0, \mathbf{Q})$: Process noise
- $\mathbf{v}_k \sim \mathcal{N}(0, \mathbf{R})$: Measurement noise

**Algorithm:**

**Predict:**
$$
\hat{\mathbf{x}}_{k|k-1} = \mathbf{F} \hat{\mathbf{x}}_{k-1|k-1} + \mathbf{B} \mathbf{u}_{k}
$$
$$
\mathbf{P}_{k|k-1} = \mathbf{F} \mathbf{P}_{k-1|k-1} \mathbf{F}^T + \mathbf{Q}
$$

**Update:**
$$
\mathbf{K}_k = \mathbf{P}_{k|k-1} \mathbf{H}^T (\mathbf{H} \mathbf{P}_{k|k-1} \mathbf{H}^T + \mathbf{R})^{-1}
$$
$$
\hat{\mathbf{x}}_{k|k} = \hat{\mathbf{x}}_{k|k-1} + \mathbf{K}_k (\mathbf{z}_k - \mathbf{H} \hat{\mathbf{x}}_{k|k-1})
$$
$$
\mathbf{P}_{k|k} = (\mathbf{I} - \mathbf{K}_k \mathbf{H}) \mathbf{P}_{k|k-1}
$$

### Implementation: 1D Kalman Filter

```python
import numpy as np

class KalmanFilter1D:
    """Simple 1D Kalman filter for position tracking."""
    
    def __init__(self, process_variance, measurement_variance, dt=0.01):
        """
        Initialize Kalman filter.
        
        State: [position, velocity]
        Measurement: [position]
        
        Args:
            process_variance: Process noise variance
            measurement_variance: Measurement noise variance
            dt: Time step
        """
        self.dt = dt
        
        # State: [x, v]
        self.x = np.array([0.0, 0.0])
        
        # Covariance
        self.P = np.eye(2) * 1.0
        
        # State transition matrix
        self.F = np.array([
            [1, dt],
            [0, 1]
        ])
        
        # Control matrix (if acceleration input exists)
        self.B = np.array([
            [0.5 * dt**2],
            [dt]
        ])
        
        # Observation matrix (measure position only)
        self.H = np.array([[1, 0]])
        
        # Process noise covariance
        self.Q = np.array([
            [dt**4/4, dt**3/2],
            [dt**3/2, dt**2]
        ]) * process_variance
        
        # Measurement noise covariance
        self.R = np.array([[measurement_variance]])
    
    def predict(self, u=0.0):
        """
        Prediction step.
        
        Args:
            u: Control input (acceleration)
        """
        # Predicted state
        self.x = self.F @ self.x + self.B.flatten() * u
        
        # Predicted covariance
        self.P = self.F @ self.P @ self.F.T + self.Q
    
    def update(self, z):
        """
        Update step.
        
        Args:
            z: Measurement (position)
        """
        # Innovation
        y = z - self.H @ self.x
        
        # Innovation covariance
        S = self.H @ self.P @ self.H.T + self.R
        
        # Kalman gain
        K = self.P @ self.H.T @ np.linalg.inv(S)
        
        # Updated state
        self.x = self.x + (K @ y).flatten()
        
        # Updated covariance
        self.P = (np.eye(2) - K @ self.H) @ self.P
    
    def get_state(self):
        """Get current state estimate."""
        return {'position': self.x[0], 'velocity': self.x[1]}


# Example usage
if __name__ == '__main__':
    kf = KalmanFilter1D(process_variance=0.1, measurement_variance=0.5, dt=0.01)
    
    # Simulate tracking
    true_pos = 0.0
    true_vel = 1.0  # Moving at 1 m/s
    
    print("Kalman Filter 1D Tracking:")
    for t in np.linspace(0, 5, 100):
        # True state
        true_pos += true_vel * 0.05
        
        # Noisy measurement
        measurement = true_pos + np.random.normal(0, 0.7)
        
        # Kalman filter
        kf.predict(u=0.0)
        kf.update(measurement)
        
        state = kf.get_state()
        
        if int(t * 20) % 10 == 0:
            print(f"t={t:.2f}s: true={true_pos:.2f}, "
                  f"measured={measurement:.2f}, "
                  f"estimated={state['position']:.2f} "
                  f"(error={abs(true_pos - state['position']):.3f})")
```

---

## ğŸ¯ Part 2: Extended Kalman Filter (EKF)

### Nonlinear Systems

**Problem**: Most robot systems are nonlinear.

**Solution**: Linearize around current estimate.

**Nonlinear Model:**
$$
\mathbf{x}_{k} = f(\mathbf{x}_{k-1}, \mathbf{u}_{k}) + \mathbf{w}_k
$$
$$
\mathbf{z}_{k} = h(\mathbf{x}_{k}) + \mathbf{v}_k
$$

**EKF Linearization:**
$$
\mathbf{F}_k = \frac{\partial f}{\partial \mathbf{x}} \bigg|_{\hat{\mathbf{x}}_{k-1}}
$$
$$
\mathbf{H}_k = \frac{\partial h}{\partial \mathbf{x}} \bigg|_{\hat{\mathbf{x}}_{k|k-1}}
$$

### Implementation: EKF for Robot Localization

```python
class ExtendedKalmanFilter:
    """EKF for 2D robot localization."""
    
    def __init__(self, dt=0.1):
        """
        State: [x, y, Î¸, v, Ï‰]
        - (x, y): Position
        - Î¸: Heading
        - v: Linear velocity
        - Ï‰: Angular velocity
        """
        self.dt = dt
        
        # State
        self.x = np.zeros(5)
        
        # Covariance
        self.P = np.eye(5) * 0.1
        
        # Process noise
        self.Q = np.diag([0.1, 0.1, 0.05, 0.5, 0.2])
        
        # Measurement noise (position + heading from vision/GPS)
        self.R = np.diag([0.5, 0.5, 0.1])
    
    def predict(self, v_cmd, w_cmd):
        """
        Prediction step (motion model).
        
        Args:
            v_cmd: Commanded linear velocity
            w_cmd: Commanded angular velocity
        """
        x, y, theta, v, w = self.x
        
        # Nonlinear motion model
        x_new = x + v * np.cos(theta) * self.dt
        y_new = y + v * np.sin(theta) * self.dt
        theta_new = theta + w * self.dt
        v_new = v + (v_cmd - v) * 0.8  # Simple velocity tracking
        w_new = w + (w_cmd - w) * 0.8
        
        self.x = np.array([x_new, y_new, theta_new, v_new, w_new])
        
        # Compute Jacobian F
        F = np.eye(5)
        F[0, 2] = -v * np.sin(theta) * self.dt
        F[0, 3] = np.cos(theta) * self.dt
        F[1, 2] = v * np.cos(theta) * self.dt
        F[1, 3] = np.sin(theta) * self.dt
        F[2, 4] = self.dt
        
        # Predict covariance
        self.P = F @ self.P @ F.T + self.Q
    
    def update(self, z):
        """
        Update step.
        
        Args:
            z: Measurement [x_measured, y_measured, theta_measured]
        """
        # Measurement model: h(x) = [x, y, Î¸]
        h = self.x[:3]
        
        # Jacobian H
        H = np.zeros((3, 5))
        H[:3, :3] = np.eye(3)
        
        # Innovation
        y = z - h
        
        # Normalize angle difference
        y[2] = np.arctan2(np.sin(y[2]), np.cos(y[2]))
        
        # Innovation covariance
        S = H @ self.P @ H.T + self.R
        
        # Kalman gain
        K = self.P @ H.T @ np.linalg.inv(S)
        
        # Update state
        self.x = self.x + K @ y
        
        # Normalize heading
        self.x[2] = np.arctan2(np.sin(self.x[2]), np.cos(self.x[2]))
        
        # Update covariance
        self.P = (np.eye(5) - K @ H) @ self.P
    
    def get_pose(self):
        """Get estimated pose [x, y, Î¸]."""
        return self.x[:3]


# Example: Robot localization with EKF
if __name__ == '__main__':
    ekf = ExtendedKalmanFilter(dt=0.1)
    
    # True robot state
    true_state = np.array([0.0, 0.0, 0.0, 0.0, 0.0])
    
    print("EKF Robot Localization:")
    for t in np.linspace(0, 10, 100):
        # Command: drive forward and turn
        v_cmd = 0.5  # m/s
        w_cmd = 0.2  # rad/s
        
        # Update true state
        true_state[0] += true_state[3] * np.cos(true_state[2]) * 0.1
        true_state[1] += true_state[3] * np.sin(true_state[2]) * 0.1
        true_state[2] += true_state[4] * 0.1
        true_state[3] += (v_cmd - true_state[3]) * 0.8
        true_state[4] += (w_cmd - true_state[4]) * 0.8
        
        # EKF prediction
        ekf.predict(v_cmd, w_cmd)
        
        # Noisy measurement (every 0.5s)
        if int(t * 10) % 5 == 0:
            z = true_state[:3] + np.random.normal(0, [0.3, 0.3, 0.05])
            ekf.update(z)
        
        if int(t * 10) % 10 == 0:
            pose = ekf.get_pose()
            error = np.linalg.norm(true_state[:2] - pose[:2])
            print(f"t={t:.1f}s: true=({true_state[0]:.2f}, {true_state[1]:.2f}), "
                  f"est=({pose[0]:.2f}, {pose[1]:.2f}), error={error:.3f}m")
```

---

## ğŸ¯ Part 3: Complete Robot State Estimator

### Fusing Multiple Sensors

**Available Sensors:**
1. **Encoders**: Joint positions (high rate, 1 kHz)
2. **IMU**: Orientation, angular velocity (500 Hz)
3. **Force/Torque**: Foot contact forces (1 kHz)
4. **Vision**: Pose estimation (30 Hz, delayed)

**Fusion Strategy:**
- High-rate: Encoders + IMU â†’ Base state
- Low-rate: Vision â†’ Drift correction
- Event-based: Force sensors â†’ Contact detection

### Implementation: Multi-Sensor State Estimator

```python
class RobotStateEstimator:
    """Complete state estimator fusing encoders, IMU, force, and vision."""
    
    def __init__(self):
        # Sub-estimators
        self.imu_filter = IMU(dt=0.002, alpha=0.98)  # From earlier
        self.ekf = ExtendedKalmanFilter(dt=0.002)
        
        # State
        self.joint_positions = np.zeros(12)  # 12 joints
        self.joint_velocities = np.zeros(12)
        self.base_orientation = np.zeros(3)  # Roll, pitch, yaw
        self.base_position = np.zeros(3)
        self.base_velocity = np.zeros(3)
        self.contact_state = [False, False]  # Left, right foot
        
        # Timing
        self.last_vision_time = 0.0
        self.vision_delay = 0.033  # 33ms camera lag
    
    def update_encoders(self, encoder_data: np.ndarray):
        """Update from joint encoders (1 kHz)."""
        # Direct measurement
        self.joint_positions = encoder_data
        
        # Compute velocities (finite difference)
        # (In practice, use filtered derivative)
        self.joint_velocities = np.diff(encoder_data, prepend=encoder_data[0]) / 0.001
    
    def update_imu(self, accel: np.ndarray, gyro: np.ndarray):
        """Update from IMU (500 Hz)."""
        self.imu_filter.update(accel, gyro)
        self.base_orientation = self.imu_filter.get_orientation()
    
    def update_force_sensors(self, left_force: np.ndarray, right_force: np.ndarray):
        """Update contact state from force sensors (1 kHz)."""
        threshold = 50.0  # Newtons
        self.contact_state[0] = np.linalg.norm(left_force) > threshold
        self.contact_state[1] = np.linalg.norm(right_force) > threshold
    
    def update_vision(self, pose_measurement: np.ndarray, timestamp: float):
        """Update from vision system (30 Hz, delayed)."""
        # Account for delay
        corrected_time = timestamp - self.vision_delay
        
        # EKF update with vision measurement
        if corrected_time > self.last_vision_time + 0.03:
            self.ekf.update(pose_measurement)
            self.base_position = self.ekf.get_pose()[:2]  # x, y
            self.last_vision_time = corrected_time
    
    def compute_base_velocity(self):
        """Estimate base velocity from kinematics and contact."""
        # If in contact, use forward kinematics from support foot
        if any(self.contact_state):
            # Simplified: velocity from joint velocities
            # (In practice, use full forward kinematics)
            self.base_velocity = np.array([
                np.mean(self.joint_velocities[:3]),
                0.0,
                0.0
            ])
        else:
            # In flight: integrate acceleration
            pass
    
    def get_full_state(self) -> dict:
        """Get complete robot state."""
        return {
            'joint_positions': self.joint_positions,
            'joint_velocities': self.joint_velocities,
            'base_position': self.base_position,
            'base_orientation': self.base_orientation,
            'base_velocity': self.base_velocity,
            'contact': self.contact_state
        }


# Example usage
if __name__ == '__main__':
    estimator = RobotStateEstimator()
    
    print("Multi-Sensor State Estimation:")
    for t in np.linspace(0, 2, 1000):
        # Simulate sensors
        encoder_data = np.random.normal(0, 0.01, 12)
        accel = np.array([0, 0, 9.81]) + np.random.normal(0, 0.1, 3)
        gyro = np.random.normal(0, 0.01, 3)
        left_force = np.array([0, 0, 300]) if t < 1.0 else np.zeros(3)
        right_force = np.array([0, 0, 300]) if t >= 1.0 else np.zeros(3)
        
        # High-rate updates
        estimator.update_encoders(encoder_data)
        estimator.update_imu(accel, gyro)
        estimator.update_force_sensors(left_force, right_force)
        
        # Low-rate vision (30 Hz)
        if int(t * 1000) % 33 == 0:
            vision_pose = np.array([t * 0.1, 0.0, 0.0])
            estimator.update_vision(vision_pose, t)
        
        estimator.compute_base_velocity()
        
        if int(t * 100) % 20 == 0:
            state = estimator.get_full_state()
            print(f"t={t:.2f}s: contact={state['contact']}, "
                  f"orientation={np.degrees(state['base_orientation'])}")
```

---

## ğŸ’» Hands-On Exercise

### ğŸ¯ Exercise: Build Complete State Estimator

**Difficulty**: â­â­â­ Advanced  
**Time**: 120 minutes

**Objective**: Implement state estimator for bipedal robot with all sensors.

**Tasks**:
1. Implement EKF for base state estimation
2. Fuse IMU, encoder, and force sensor data
3. Add vision updates for drift correction
4. Handle contact switching
5. Validate with simulated walking

**Success Criteria**:
- âœ… Position error < 5 cm over 10 second walk
- âœ… Orientation error < 2Â°
- âœ… Correct contact detection (>95% accuracy)
- âœ… Real-time performance (>500 Hz)

---

## ğŸ§  Key Takeaways

1. **Sensor fusion** combines complementary sensor strengths
2. **Kalman filter** is optimal for linear Gaussian systems
3. **EKF** handles nonlinear dynamics through linearization
4. **Multi-rate fusion** uses sensors at their native rates
5. **Contact detection** critical for legged robot estimation

---

## ğŸ“š Further Reading

- **Book**: "Probabilistic Robotics" (Thrun, Burgard, Fox)
- **Paper**: "State Estimation for Legged Robots" (Bloesch et al., 2013)
- **Tutorial**: [Kalman Filter Interactive](https://www.kalmanfilter.net/default.aspx)

---

## â¡ï¸ Next Module

Completed Module 3! Ready for AI integration?

**[Module 4: AI Integration â†’](../../module-4/)**

Learn deep learning and reinforcement learning for robots.

---

<ChatbotPlaceholder />
